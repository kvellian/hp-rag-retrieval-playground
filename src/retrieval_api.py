
# Auto-generated by Notebook 4
import re, joblib, numpy as np, scipy.sparse as sp, faiss
from pathlib import Path
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd, yaml

BASE_DIR = Path("/content/drive/MyDrive/hp_rag")
PROC_DIR = BASE_DIR / "processed"
IDX_DIR  = BASE_DIR / "indexes"
EMB_DIR  = BASE_DIR / "embeddings"
CFG      = BASE_DIR / "config" / "retrieval.yml"

EMBED_MODELS = {
  "minilm_l6": "sentence-transformers/all-MiniLM-L6-v2",
  "e5_small": "intfloat/e5-small-v2",
  "bge_base": "BAAI/bge-base-en-v1.5"
}

# load chunks
if (PROC_DIR / "hp_chunks.parquet").exists():
    chunks = pd.read_parquet(PROC_DIR / "hp_chunks.parquet")
elif (PROC_DIR / "hp_chunks.csv").exists():
    chunks = pd.read_csv(PROC_DIR / "hp_chunks.csv")
else:
    raise FileNotFoundError("No processed chunks found.")

# load tf-idf
vectorizer = joblib.load(IDX_DIR / "tfidf_vectorizer.pkl")
X_tfidf    = sp.load_npz(IDX_DIR / "X_tfidf.npz")

# load config
if CFG.exists():
    with open(CFG, "r", encoding="utf-8") as f:
        _cfg = yaml.safe_load(f) or {}
else:
    _cfg = {}

DEFAULTS = _cfg.get("defaults", {})
ALPHAS   = _cfg.get("alphas", {k: 0.6 for k in EMBED_MODELS})

BONUS1  = float(DEFAULTS.get("title_bonus_1tok", 0.05))
BONUS2  = float(DEFAULTS.get("title_bonus_2tok", 0.05))
ENTITY_BOOST = float(DEFAULTS.get("entity_boost", 1.08))
SHORT_PENALTY_LEN = int(DEFAULTS.get("short_penalty_len", 80))
SHORT_PENALTY     = float(DEFAULTS.get("short_penalty", 0.95))
TOP_M             = int(DEFAULTS.get("top_m", 200))

def _prep_query_text(key: str, q: str) -> str:
    if key.startswith("e5_") or key.startswith("bge_"):
        return f"query: {q}"
    return q

def _minmax(x: np.ndarray) -> np.ndarray:
    x = np.asarray(x, dtype=np.float32)
    rng = np.ptp(x)
    if rng == 0.0:
        return np.zeros_like(x)
    return (x - x.min()) / (rng + 1e-9)

_loaded = {}
def _load_faiss_and_model(key="minilm_l6"):
    idx = faiss.read_index(str(IDX_DIR / f"faiss_{key}.index"))
    model = SentenceTransformer(EMBED_MODELS[key])
    return idx, model

def search_hybrid(query, k=5, alpha=None, key="minilm_l6", m=None, enable_house_bias=False):
    """Hybrid semantic + lexical search.

    Args:
        query (str): User question.
        k (int): How many results to RETURN.
        alpha (float or None): Semantic vs lexical weight. If None, use config.
        key (str): Embedding model key (e.g., 'minilm_l6', 'e5_small', 'bge_base').
        m (int or None): How many FAISS candidates to pull BEFORE re-ranking.
                         If None, uses max(k, TOP_M).
    """
    if alpha is None:
        alpha = float(ALPHAS.get(key, 0.6))

    if key not in _loaded:
        _loaded[key] = _load_faiss_and_model(key)
    index, embed_model = _loaded[key]

    # decide how many neighbors to retrieve from FAISS
    if m is None:
        m = TOP_M
    m = int(max(m, k))  # always at least k, and always an int

    # semantic
    q_sem = _prep_query_text(key, query)
    qv = embed_model.encode([q_sem], normalize_embeddings=True).astype("float32")
    D, I = index.search(qv, m)
    I = I[0]; sem = D[0].astype(np.float32)

    # lexical
    qX = vectorizer.transform([query])
    lex = cosine_similarity(qX, X_tfidf[I])[0].astype(np.float32)

    # title/entity tweaks
    q_tokens = set(re.findall(r"\w+", query.lower()))
    titles = (chunks.iloc[I]["title"].fillna("").str.lower()
              .str.findall(r"\w+").apply(set))
    overlaps = titles.apply(lambda s: len(q_tokens & s)).to_numpy()
    title_bonus = (overlaps > 0).astype(float) * BONUS1 + (overlaps > 1).astype(float) * BONUS2
    entity_boost = np.where(overlaps > 0, ENTITY_BOOST, 1.0).astype(np.float32)

    # short penalty
    short_mask = (chunks.iloc[I]["n_words"].to_numpy() < SHORT_PENALTY_LEN)
    penalties = np.where(short_mask, SHORT_PENALTY, 1.0).astype(np.float32)

    s_sem = _minmax(sem); s_lex = _minmax(lex)
    final = (alpha * s_sem + (1 - alpha) * s_lex + title_bonus) * penalties * entity_boost

    hits = chunks.iloc[I].copy()
    hits["sem"] = sem
    hits["lex"] = lex
    hits["title_bonus"] = title_bonus
    hits["penalty"] = penalties
    hits["entity_boost"] = entity_boost
    hits["final_score"] = final

    return hits.sort_values("final_score", ascending=False).head(k).reset_index(drop=True)
